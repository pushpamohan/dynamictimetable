{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of bisect_l.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushpamohan/dynamictimetable/blob/master/Copy_of_Copy_of_bisect_l.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-WT5mm_Gy0z",
        "colab_type": "text"
      },
      "source": [
        "# memory check -GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbQ3B5lNcd7X",
        "colab_type": "code",
        "outputId": "5fe460a0-b065-4219-eb3e-643a1c047368",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7411 sha256=eeb738927d4986fe78ee6810d5be83eaa40dae4e52b8a05315072add22cb478e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.9 GB  | Proc size: 154.2 MB\n",
            "GPU RAM Free: 15079MB | Used: 0MB | Util   0% | Total 15079MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5Lz4pz0HNjf",
        "colab_type": "text"
      },
      "source": [
        "## install pyspark\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38k1eslXch7S",
        "colab_type": "code",
        "outputId": "ca74940d-43dc-438e-81da-c75ac58cfaac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/98/244399c0daa7894cdf387e7007d5e8b3710a79b67f3fd991c0b0b644822d/pyspark-2.4.3.tar.gz (215.6MB)\n",
            "\u001b[K     |████████████████████████████████| 215.6MB 139kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 45.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.3-py2.py3-none-any.whl size=215964963 sha256=cd69d67b61dae747380a339816a56472b1e91c608b809e44a3337c4cdf8bc51c\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/20/f0/b30e2024226dc112e256930dd2cd4f06d00ab053c86278dcf3\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kluJEny9HfIX",
        "colab_type": "text"
      },
      "source": [
        "# **install jdk hadoop  spark**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqat7UXHcoXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.3-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlUTR366Hvt2",
        "colab_type": "text"
      },
      "source": [
        "#**## load dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plTHr5qscpVj",
        "colab_type": "code",
        "outputId": "4e7c6630-1614-43b2-b880-c4414dca5a87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "!curl -O http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
        "!unzip ml-latest-small.zip\n",
        "!cd ml-latest-small/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  955k  100  955k    0     0   584k      0  0:00:01  0:00:01 --:--:--  584k\n",
            "Archive:  ml-latest-small.zip\n",
            "   creating: ml-latest-small/\n",
            "  inflating: ml-latest-small/links.csv  \n",
            "  inflating: ml-latest-small/tags.csv  \n",
            "  inflating: ml-latest-small/ratings.csv  \n",
            "  inflating: ml-latest-small/README.txt  \n",
            "  inflating: ml-latest-small/movies.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZCEwPS6H8xc",
        "colab_type": "text"
      },
      "source": [
        "# set environmental varible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNLd5Km0cuge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.3-bin-hadoop2.7\"\n",
        "# spark imports\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import UserDefinedFunction, explode, desc\n",
        "from pyspark.sql.types import StringType, ArrayType\n",
        "from pyspark.mllib.recommendation import ALS\n",
        "\n",
        "# data science imports\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# visualization imports\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IYAonLHILIG",
        "colab_type": "text"
      },
      "source": [
        "# **set path**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IGNV-okIPsF",
        "colab_type": "text"
      },
      "source": [
        "## **set session**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptoZ_8gMcve_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export PATH=$SPARK_HOME/bin:$PATH\n",
        "!export PYSPARK_PYTHON=python3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7iK0jPlcypB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# spark config\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"bisect_l\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"96g\") \\\n",
        "    .config(\"spark.driver.memory\", \"96g\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .config(\"spark.master\", \"local[12]\") \\\n",
        "    .getOrCreate()\n",
        "# get spark context\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMuV2ZlVIWkN",
        "colab_type": "text"
      },
      "source": [
        "## **read data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLfHyN8Nc8-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies = spark.read.load(os.path.join('/content/ml-latest-small', 'movies.csv'), format='csv', header=True, inferSchema=True)\n",
        "ratings = spark.read.load(os.path.join('/content/ml-latest-small', 'ratings.csv'),format='csv', header=True, inferSchema=True)\n",
        "links = spark.read.load(os.path.join('/content/ml-latest-small','links.csv'), format='csv', header=True, inferSchema=True)\n",
        "tags = spark.read.load(os.path.join('/content/ml-latest-small','tags.csv'), format='csv', header=True, inferSchema=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op6R6DGDIdjC",
        "colab_type": "text"
      },
      "source": [
        "### **load data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV8HADgndGXH",
        "colab_type": "code",
        "outputId": "cdeea283-eab5-42c0-d269-c72ead6aa40d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# load data\n",
        "movie_rating = sc.textFile(os.path.join('/content/ml-latest-small', 'ratings.csv'))\n",
        "# preprocess data -- only need [\"userId\", \"movieId\", \"rating\"]\n",
        "header = movie_rating.take(1)[0]\n",
        "rating_data = movie_rating \\\n",
        "    .filter(lambda line: line!=header) \\\n",
        "    .map(lambda line: line.split(\",\")) \\\n",
        "    .map(lambda tokens: (int(tokens[0]), int(tokens[1]), float(tokens[2]))) \\\n",
        "    .cache()\n",
        "# check three rows\n",
        "rating_data.take(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 1, 4.0), (1, 3, 4.0), (1, 6, 4.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0U8UzMJIkdV",
        "colab_type": "text"
      },
      "source": [
        "## **split data into train ,test and validate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WU7byQDZdNQU",
        "colab_type": "code",
        "outputId": "62eec7c7-c4e5-4a02-8619-dc9c8f9e8b0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train, validation, test = rating_data.randomSplit([7, 1, 2], seed=99)\n",
        "# cache data\n",
        "train.cache()\n",
        "validation.cache()\n",
        "test.cache()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[49] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj-isqogDE6y",
        "colab_type": "text"
      },
      "source": [
        "# **Bisect K-Means**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJhUOH3_deaF",
        "colab_type": "code",
        "outputId": "25bbe752-3e90-4ee6-c1c0-592c6b9a8d43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from numpy import array\n",
        "# $example off$\n",
        "\n",
        "from pyspark.mllib.clustering import BisectingKMeans\n",
        "\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "# $example off$\n",
        "    # Build the model (cluster the data)\n",
        "model = BisectingKMeans.train(rating_data, 3, maxIterations=10)\n",
        "predictions = model.predict(rating_data)\n",
        "print(predictions)\n",
        "\n",
        "        # Evaluate clustering\n",
        "cost = model.computeCost(rating_data)\n",
        "print(\"Bisecting K-means Cost = \" + str(cost))\n",
        "    # $example off$ \n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MapPartitionsRDD[1222] at mapPartitions at PythonMLLibAPI.scala:1336\n",
            "Bisecting K-means Cost = 21163097895341.08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltJ9uK7cC9tM",
        "colab_type": "text"
      },
      "source": [
        "# K-means "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "622OHYCZCqKy",
        "colab_type": "code",
        "outputId": "de17f345-8e2a-4977-f7ee-2e59b0fe5f8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "from math import sqrt\n",
        "\n",
        "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
        "\n",
        "\n",
        "# Build the model (cluster the data)\n",
        "clusters = KMeans.train(rating_data, 3, maxIterations=10, initializationMode=\"random\")\n",
        "\n",
        "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
        "def error(point):\n",
        "    center = clusters.centers[clusters.predict(point)]\n",
        "    return sqrt(sum([x**2 for x in (point - center)]))\n",
        "\n",
        "WSSSE = rating_data.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
        "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
        "\n",
        "model = KMeans.train(rating_data, 2)\n",
        "print(\"Final centers: \" + str(model.clusterCenters))\n",
        "print(\"Total Cost: \" + str(model.computeCost(rating_data)))\n",
        "\n",
        "# Save and load model\n",
        "#clusters.save(sc, \"bisect_l/KMeansModel\")\n",
        "#sameModel = KMeansModel.load(sc, \"bisect_l//KMeansModel\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Within Set Sum of Squared Error = 516417653.21726453\n",
            "Final centers: [array([3.26031835e+02, 4.57665585e+03, 3.49835021e+00]), array([3.26565201e+02, 8.73640251e+04, 3.51621726e+00])]\n",
            "Total Cost: 25525963320001.617\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}